{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5110, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>ever_married</th>\n",
       "      <th>work_type</th>\n",
       "      <th>Residence_type</th>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <th>bmi</th>\n",
       "      <th>smoking_status</th>\n",
       "      <th>stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9046</td>\n",
       "      <td>male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>private</td>\n",
       "      <td>urban</td>\n",
       "      <td>228.69</td>\n",
       "      <td>36.6</td>\n",
       "      <td>formerly smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51676</td>\n",
       "      <td>female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>rural</td>\n",
       "      <td>202.21</td>\n",
       "      <td>28.1</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31112</td>\n",
       "      <td>male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>private</td>\n",
       "      <td>rural</td>\n",
       "      <td>105.92</td>\n",
       "      <td>32.5</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60182</td>\n",
       "      <td>female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>private</td>\n",
       "      <td>urban</td>\n",
       "      <td>171.23</td>\n",
       "      <td>34.4</td>\n",
       "      <td>smokes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1665</td>\n",
       "      <td>female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>self-employed</td>\n",
       "      <td>rural</td>\n",
       "      <td>174.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>never smoked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
       "0   9046    male  67.0             0              1          yes   \n",
       "1  51676  female  61.0             0              0          yes   \n",
       "2  31112    male  80.0             0              1          yes   \n",
       "3  60182  female  49.0             0              0          yes   \n",
       "4   1665  female  79.0             1              0          yes   \n",
       "\n",
       "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
       "0        private          urban             228.69  36.6  formerly smoked   \n",
       "1  self-employed          rural             202.21  28.1     never smoked   \n",
       "2        private          rural             105.92  32.5     never smoked   \n",
       "3        private          urban             171.23  34.4           smokes   \n",
       "4  self-employed          rural             174.12  24.0     never smoked   \n",
       "\n",
       "   stroke  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Import libraries and load cleaned dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StrastifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, roc_curve)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the cleaned datasetfrom imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = ImbPipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('smote', SMOTE(random_state=42)),   # <-- resamples minority class only in training folds\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "DATA_PATH = Path(\"../data/stroke_clean.csv\")  # adjust if path differs\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "# df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (4088, 10)  Test shape: (1022, 10)\n",
      "Stroke ratio (train): stroke\n",
      "0    0.951321\n",
      "1    0.048679\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define features and target\n",
    "\n",
    "target = 'stroke'\n",
    "\n",
    "feature_columns = [\n",
    "    'gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
    "    'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status'\n",
    "]\n",
    "\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target].astype(int)\n",
    "\n",
    "# Stratified split to preserve class ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Stroke ratio (train):\", y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build preprocessing pipeline\n",
    "\n",
    "numeric_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "categorical_features = [\n",
    "    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "]\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_features),\n",
    "        ('cat', cat_pipeline, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Addressing Class Imbalance - increase the penalty for misclassification in logistic regression\n",
    "# - We keep the original data intact and therefore no artificial sampling\n",
    "# 3 training techgniques as proposed with respect to Data Imbalance we have: \n",
    "# - Class Weighting for Logistic Regression, SMOTE technique,XGBoosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Logistic Regression with class weighting\n",
    "logit_clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight='balanced',   # <-- handles imbalance internally\n",
    "        solver='liblinear'\n",
    "    ))\n",
    "])\n",
    "# model was trained using built-in class weighting to offset the 5 % stroke prevalence. \n",
    "# This method ensures the minority class contributes proportionally to the loss function without resampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 SMOTE Oversampling (for Random Forest or other non-weighted models)\n",
    "# Random Forest classifier was trained within an imbalanced-learning pipeline that applied Synthetic Minority Oversampling Technique (SMOTE) to the training folds, - Preventing data leakage \n",
    "# Ensures balanced representation of stroke and non-stroke cases during training.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = ImbPipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('smote', SMOTE(random_state=42)),   # <-- resamples minority class only in training folds\n",
    "    ('model', RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Scale Pos Weight (for XGBoost)\n",
    "# - tells the booster to weight positive examples more.\n",
    "# XGBoost is sensitive to class imbalance but supports this natively.\n",
    "# the class imbalance was addressed through the scale_pos_weight parameter, calculated as the ratio of negative to positive samples in the training set. \n",
    "# This ensures the boosting algorithm penalizes missed stroke cases more heavily.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# compute weight ratio\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "xgb_clf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        scale_pos_weight=scale_pos_weight   # <-- imbalance compensation\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stroke_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ---------- Load cleaned data ----------\u001b[39;00m\n\u001b[32m     28\u001b[39m DATA_PATH = Path(\u001b[33m\"\u001b[39m\u001b[33mstroke_clean.csv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# adjust if different\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m target = \u001b[33m\"\u001b[39m\u001b[33mstroke\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m feature_columns = [\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgender\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mhypertension\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mheart_disease\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mever_married\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwork_type\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mResidence_type\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mavg_glucose_level\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mbmi\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msmoking_status\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py312/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py312/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py312/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py312/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/py312/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'stroke_clean.csv'"
     ]
    }
   ],
   "source": [
    "# STEP 5: Train, handle imbalance, evaluate, and compare three models\n",
    "# Models: Logistic Regression (class_weight), RandomForest (+SMOTE), XGBoost (scale_pos_weight)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score, average_precision_score,\n",
    "                             precision_recall_curve, roc_curve, f1_score)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# If xgboost isn't installed, run: pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ---------- Load cleaned data ----------\n",
    "DATA_PATH = Path(\"../data/stroke_clean.csv\")  # adjust if different\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "target = \"stroke\"\n",
    "feature_columns = [\n",
    "    \"gender\",\"age\",\"hypertension\",\"heart_disease\",\"ever_married\",\n",
    "    \"work_type\",\"Residence_type\",\"avg_glucose_level\",\"bmi\",\"smoking_status\"\n",
    "]\n",
    "X = df[feature_columns].copy()\n",
    "y = df[target].astype(int).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- Preprocessing (leak-proof) ----------\n",
    "numeric_features = [\"age\",\"avg_glucose_level\",\"bmi\"]\n",
    "categorical_features = list(set(feature_columns) - set(numeric_features))\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, numeric_features),\n",
    "    (\"cat\", cat_pipeline, categorical_features),\n",
    "])\n",
    "\n",
    "# ---------- Class imbalance helpers ----------\n",
    "pos = int((y_train == 1).sum())\n",
    "neg = int((y_train == 0).sum())\n",
    "scale_pos_weight = neg / max(pos, 1)  # for XGB\n",
    "\n",
    "# ---------- Define three models ----------\n",
    "models = {\n",
    "    \"Logistic (balanced)\": Pipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"liblinear\"))\n",
    "    ]),\n",
    "    \"RandomForest + SMOTE\": ImbPipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1))\n",
    "    ]),\n",
    "    \"XGBoost (SPW)\": Pipeline([\n",
    "        (\"pre\", preprocess),\n",
    "        (\"clf\", XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=42,\n",
    "            eval_metric=\"logloss\",\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# ---------- Metric helpers ----------\n",
    "def pr_auc(y_true, y_prob):\n",
    "    return average_precision_score(y_true, y_prob)\n",
    "\n",
    "def best_threshold_for_fbeta(y_true, y_prob, beta=2.0):\n",
    "    \"\"\"Return threshold that maximizes F-beta (default F2: recall-weighted).\"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    thresholds = np.append(thresholds, 1.0)  # align lengths\n",
    "    beta2 = beta**2\n",
    "    fbeta = (1+beta2) * (precisions*recalls) / (beta2*precisions + recalls + 1e-12)\n",
    "    idx = np.nanargmax(fbeta)\n",
    "    return float(thresholds[idx]), float(fbeta[idx]), float(precisions[idx]), float(recalls[idx])\n",
    "\n",
    "def print_eval(y_true, y_prob, model_name, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    print(f\"\\n=== {model_name} @ threshold={threshold:.2f} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"{model_name} — Confusion Matrix (thr={threshold:.2f})\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Train, evaluate, compare ----------\n",
    "metrics_rows = []\n",
    "roc_curves = []\n",
    "pr_curves = []\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    # cross-val ROC-AUC on train (threshold-free)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_auc = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"roc_auc\").mean()\n",
    "\n",
    "    # fit and predict proba\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # threshold-free metrics on test\n",
    "    roc = roc_auc_score(y_test, y_prob)\n",
    "    pr = pr_auc(y_test, y_prob)\n",
    "\n",
    "    # pick recall-favoring threshold by maximizing F2\n",
    "    thr_star, f2_star, p_star, r_star = best_threshold_for_fbeta(y_test, y_prob, beta=2.0)\n",
    "\n",
    "    # store metrics\n",
    "    metrics_rows.append({\n",
    "        \"model\": name,\n",
    "        \"cv_roc_auc_train\": round(cv_auc, 4),\n",
    "        \"roc_auc_test\": round(roc, 4),\n",
    "        \"pr_auc_test\": round(pr, 4),\n",
    "        \"best_thr_F2\": round(thr_star, 3),\n",
    "        \"F2_at_best_thr\": round(f2_star, 4),\n",
    "        \"precision_at_best_thr\": round(p_star, 4),\n",
    "        \"recall_at_best_thr\": round(r_star, 4),\n",
    "    })\n",
    "\n",
    "    # plot reports at default and tuned thresholds\n",
    "    print_eval(y_test, y_prob, name, threshold=0.50)\n",
    "    print_eval(y_test, y_prob, name, threshold=thr_star)\n",
    "\n",
    "    # cache curves for combined plots\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    roc_curves.append((name, fpr, tpr, roc))\n",
    "    pr_curves.append((name, rec, prec, pr))\n",
    "\n",
    "# ---------- Metrics table ----------\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(by=\"pr_auc_test\", ascending=False)\n",
    "metrics_df\n",
    "Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "metrics_df.to_csv(\"reports/model_metrics.csv\", index=False)\n",
    "\n",
    "# ---------- Combined ROC plot ----------\n",
    "plt.figure(figsize=(6,5))\n",
    "for name, fpr, tpr, roc in roc_curves:\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc:.3f})\")\n",
    "plt.plot([0,1],[0,1],'--',linewidth=1)\n",
    "plt.title(\"ROC Curve (Test)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------- Combined PR plot ----------\n",
    "plt.figure(figsize=(6,5))\n",
    "for name, rec, prec, pr in pr_curves:\n",
    "    plt.plot(rec, prec, label=f\"{name} (PR-AUC={pr:.3f})\")\n",
    "plt.title(\"Precision–Recall Curve (Test)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved metrics to: reports/model_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
